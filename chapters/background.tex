%TEX root = ../dissertation.tex

\chapter{Background}
\label{chapter:background}

\section{The basics of General Game Playing}

General Game Playing is a project of the Stanford Logic Group of Stanford University, California, which aims to create a platform for \gls{GGP}.
Since 2005, there have been annual \gls{GGP} competitions at the AAAI Conference.

A \gls{GGP} match consists of 3 major components:
\begin{itemize}
\item Game Description: The game rules, in \gls{GDL}.

\item Game Manager: This system acts as a referee and manages communication with the players and other systems like graphics for the spectators. \textit{State Data} is usually part of the Game Manager.

\item Players: Players are the most interesting component of a \gls{GGP} game, they need.

\end{itemize}

\begin{figure}[h]
	\centering
    \includegraphics[scale=0.8]{images/GGPgamesetup.png}
    \caption{Match Components}
    \label{fig:match components}
\end{figure}

At the beginning of a match, the Game Manager sends all Players a match identifier, the game description, the role of each player and the time limits (for preparation (\textit{startclock}) and for each round (\textit{playclock})).

The match commences, after all players respond, with the Game Manager requesting a move from all players. Each round ends when all players send their moves or the time limit runs out (a random legal move is chosen for players that don't respond in time), after which the Game Manager will send each player another request along with all moves taken in the previous round.


\subsection{Game Description Language}
The \gls{GDL} is the standard way of describing games in the \gls{GGP} community.
\gls{GGP} players interpret the language using something called a \textit{Reasoner}. Choosing a good way of interpreting the game rules is one of the keys to performance and so many players develop their own custom \textit{reasoners}.

It can describe any finite deterministic move-based strategy game with an arbitrary number of players (most board games). GDL-II is an extension that has been made to allow for probabilistic games and incomplete information, like most card games.

Both GDL and GDL-II are variants of Datalog (query and rule language similar to prolog) and use first order logic.
Since GDL is a very conceptual description of the rules their interpretation is very computationally expensive. Choosing a good way of doing this interpretation (components that do this are called reasoners) is therefore very important to player performance, even in the recent years.

An example of tic tac toe described in GDL with some syntax explanation can be seen in \ref{appendix:gdl_example}

\subsection{Game Manager}
The purpose of the Game Manager is to be a single source of truth about what's happening in a match, and verify all moves taken by players. It must be able to interpret \gls{GDL}, to verify these moves

Players communicate their moves to the Game Manager (via HTTP), who checks the validity of the moves. A random legal move is chosen if a player chooses an illegal move or doesn't respond in time.

It should also provide a way of archiving the match history (all game states and moves taken) and other useful features like an interface for spectating.

\subsection{Game Player}
Game Players are systems that can interpret a \gls{GDL} game description, communicate with the Game Manager and devise strategies, to maximize their result in a certain game.

Game Players are, of course, the most interesting part of any \gls{GGP} match. Their aim is to be as general as possible while also having reasonably good performance in any game, which is a surprisingly difficult feat. Suffice it to say, sophisticated AI techniques like heuristics are very hard to successfully apply in a general, domain independent, way. So far, 
The most relevant techniques are discussed in chapter \ref{chapter:state_of_the_art}.

\section{Player components}

\subsection{Markov Decision Problem}

\section{Techniques}
\subsection{Monte Carlo Tree Search}
Introduced to the GGP competition by CADIA player in 2007, It’s currently the most used and successful method in GGP. Starting from the current state, the algorithm traverses the tree until the move timer ends, doing as many iterations as possible.

Each iteration has four steps: selection, expansion, simulation and back propagation:

\begin{enumerate}

\item Selection: Some technique is used to select which already traversed node to start from for a search. The most common technique is Upper Confidence Bounds applied for Trees (UCT), which includes a constant that can be tweaked to favor more or less exploration of non visited branches. The UCT algorithm is described in \ref{UCT}:

\begin{center}
\begin{equation} \label{UCT}
a^{*} = arg \max_{a\in A(s)} \left \{ Q(s,a) + C \sqrt{\frac{\ln|N(s)|} {N(s,a)}} \right \}
\end{equation}
\end{center}

Where $a^{*}$ is the selected node, $a \in A(s)$ means an action that contained in the set of possible actions in the current state $s$, $Q(s,a)$ is an assessment of performing $a$ in state $s$, $C$ is the exploration ratio constant, $N(s)$ is the number of previous visits to state $s$ and $N(s,a)$ is the number of times $a$ has been sampled in state $s$.

\item Expansion: Adding a node with the first unvisited state yet to the tree, meaning a state that wasn’t already in the tree.

\item Simulation: Perform a random simulation until a terminal game state is reached.

\item Back-Propagation: The scores obtained by all players at the end of the simulation are back-propagated to all nodes traveled in the selection and expansion stages.

\end{enumerate}

The success of MCTS can be mostly attributed to it not requiring any game-specific knowledge, although this can become a problem if other techniques like heuristics become advanced enough at learning important features of games, as heuristic search can be much faster than simulation. MCTS also has the advantage of parallelizing well. The biggest problems for MCTS are games that can have infinite moves without ending and tree size.

There have been several suggested improvements to the basic MCTS, although most aren’t very thoroughly tested yet. One of the most interesting ones is Simulation Heuristics, proposed by MINI-Player, which aims to add some sort of learning to the standard MCTS algorithm. The heuristics proposed are very light-weight and are the following:

\begin{itemize}

\item Random: The standard MCTS

\item History Heuristic: Tries to identify globally good actions (generally good regardless of state)

\item Mobility: Favors actions that lead to states with more move options relatively to other players.

\item Approximate Goal Evaluation: Tries to calculate the degree of satisfaction of a GDL goal rule. 

\item Exploration: Measures the difference between states as a way to do a diverse exploration. 

\item Statistical Symbol Counting: Before the start clock simulations are done to calculate the correlation between game score and certain game symbols (moves, pieces, board locations, etc). Symbols that do not change much are then ignored to allow more computation to be made on the more relevant ones.

\end{itemize}

\subsection{AI-Based Techniques}
Although currently not competitive with MCTS, there are several different approaches more similar to classical AI. Some of these are attempts at multi-game playing that predate GGP. These are techniques that try to learn or identify features of the game. One of the biggest drawbacks of this type of technique is that no general heuristic exists, meaning heuristics have to be discovered at run time, which is a very complex problem.

